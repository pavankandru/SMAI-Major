{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_viz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhgTCqM4L7MG"
      },
      "source": [
        "# Model and Data Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKeAY-UO0Dvg"
      },
      "source": [
        "# Change the paths as you need\n",
        "!mkdir /content/models\n",
        "!mkdir /content/models/vgg16\n",
        "!mkdir /content/models/alexnet\n",
        "!wget https://download.pytorch.org/models/vgg16-397923af.pth -O /content/models/vgg16/model.pth\n",
        "!wget https://download.pytorch.org/models/alexnet-owt-7be5be79.pth -O /content/models/alexnet/model.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hYg-s_wFhgM"
      },
      "source": [
        "!mkdir /content/models/resnet\n",
        "!wget https://download.pytorch.org/models/resnet50-0676ba61.pth -O /content/models/resnet/model.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx95sAzD80_R"
      },
      "source": [
        "#Optional you can get images from anywhere you want\n",
        "!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
        "!tar -xf /content/images.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC83rb9QL9o7"
      },
      "source": [
        "# Load Input Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T4KjzDNiIY_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ARWblOr6i5v"
      },
      "source": [
        "from PIL import Image,ImageEnhance\n",
        "from torchvision import transforms\n",
        "input_image = Image.open('test_image.jpg')#('/content/Images/n02089867-Walker_hound/n02089867_1105.jpg')\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "# Try with and without normalizing\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "transforms.ToPILImage()(input_batch[0]).convert(\"RGB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k7QamGiMBSc"
      },
      "source": [
        "# Deconv Wrapper Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqsbiw6ZdFhu"
      },
      "source": [
        "class Deconv(nn.Module):\n",
        "  def __init__(self,model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.layer_counter = 0\n",
        "    self.pass_forward = 0\n",
        "\n",
        "  def forward_one_layer(self,x):\n",
        "    # print('At layer ',self.layer_counter)\n",
        "    layer, rev_layer = self.model.features[self.layer_counter],self.model.rev_features[self.layer_counter]\n",
        "    if layer._get_name()=='MaxPool2d':\n",
        "      y,switches = layer(x)\n",
        "      switches = switches[0].repeat(18,1,1,1)\n",
        "    else:\n",
        "      y,switches = layer(x),None\n",
        "    \n",
        "    if self.pass_forward==0:\n",
        "      with torch.no_grad():\n",
        "        norms = [i.norm() for i in y[0]]\n",
        "        ordered_indexes = np.argsort(norms)\n",
        "      plt.figure(figsize=(20,10))############\n",
        "      count=0###############################\n",
        "      back_input = []\n",
        "      for ind in ordered_indexes[-18:][::-1]:\n",
        "        new_y = y[0].detach().clone()\n",
        "        plt.subplot(3,6,count+1)#####################################\n",
        "        vis_mean = transforms.ToPILImage()(new_y[ind].clamp(min=0)).convert(\"RGB\")\n",
        "        enhancer = ImageEnhance.Contrast(vis_mean)\n",
        "        factor = 2\n",
        "        enh_vis_mean = enhancer.enhance(factor)\n",
        "        plt.imshow(enh_vis_mean)\n",
        "        count+=1\n",
        "        plt.axis('off')#####################################\n",
        "        new_y = torch.stack([i if index==ind else torch.zeros_like(i) for index,i in enumerate(new_y)],axis=0)\n",
        "\n",
        "        back_input.append(new_y)\n",
        "      back_input = torch.stack(back_input,axis=0)\n",
        "      plt.subplots_adjust(wspace=0, hspace=0)\n",
        "      plt.show()\n",
        "      \n",
        "\n",
        "      if switches is not None:\n",
        "        return rev_layer(back_input,switches)\n",
        "      return rev_layer(back_input)\n",
        "    self.layer_counter+=1\n",
        "    self.pass_forward-=1\n",
        "    back_input = self.forward_one_layer(y)\n",
        "    if switches is not None:\n",
        "      return rev_layer(back_input,switches)\n",
        "    return rev_layer(back_input)\n",
        "\n",
        "      \n",
        "  def forward(self,x,j = 1):\n",
        "    self.pass_forward = j -1\n",
        "    self.layer_counter = 0\n",
        "    viss = self.forward_one_layer(x)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for i,vis in enumerate(viss):\n",
        "      plt.subplot(3,6,i+1)\n",
        "      # Try out various clipping/normalization methods\n",
        "      vis_mean = transforms.ToPILImage()(vis.clamp(min=0)).convert(\"RGB\")\n",
        "      # ToDo try out various enhancing methods\n",
        "      enhancer = ImageEnhance.Contrast(vis_mean)\n",
        "      factor = 2\n",
        "      enh_vis_mean = enhancer.enhance(factor)\n",
        "      plt.imshow(enh_vis_mean)\n",
        "      plt.axis('off')\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    plt.show()\n",
        "dvgg = Deconv(vgg16)\n",
        "dvgg(input_batch,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXSGBVwAKUJE"
      },
      "source": [
        "# VGG 16 (without batch normalization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP4VgFI-hwwa"
      },
      "source": [
        "%run vgg.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tASNvnwDeow8"
      },
      "source": [
        "for i in range(1,11):\n",
        "  print('Activations and Features at layer ',i)\n",
        "  dvgg = Deconv(vgg16)\n",
        "  dvgg(input_batch,i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2I-ShgwvZHz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjKFCxghKO8M"
      },
      "source": [
        "# alexnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YAzRQB71MBN"
      },
      "source": [
        "%run alexnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EVAURhk1vF1"
      },
      "source": [
        "for i in range(1,len(an.features)+1):\n",
        "  print('Features at layer ',i)\n",
        "  dvgg = Deconv(vgg16)\n",
        "  dvgg(input_batch,i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLMGu9O9RPfC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo3jiGMUJ6B-"
      },
      "source": [
        "# ResNet 50\n",
        "Deconvolution method is not described for skip connections and batch normalization hence using Vanilla Backprop instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgcHedbtGzyy"
      },
      "source": [
        "%run resnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pst0X8HZMHwT"
      },
      "source": [
        "## Backprop Wrapper Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn9pCABWGzvW"
      },
      "source": [
        "# Hooks are a bit weird. Every time you make a change restart the notebook you don't need to download all the files everytime\n",
        "class VanillaBackprop(nn.Module):\n",
        "  def __init__(self,model,threshold_for_vis=0):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.layer_counter =0 \n",
        "    try:\n",
        "      count=0\n",
        "      for layer in self.model.features:\n",
        "        if count>=15:\n",
        "            break\n",
        "        if layer._get_name()=='Conv2d':\n",
        "          count+=1\n",
        "          print('Registering hook on ',layer)\n",
        "          layer.register_forward_hook(self.hook)\n",
        "    except:\n",
        "      l = [module for module in self.model.modules() if type(module) != nn.Sequential]\n",
        "      count=0\n",
        "      for layer in l:\n",
        "        if count>=15:\n",
        "          break\n",
        "        if layer._get_name()=='Conv2d':\n",
        "          count+=1\n",
        "          print('Registering hook on ',layer)\n",
        "          layer.register_forward_hook(self.hook)\n",
        "        \n",
        "\n",
        "  def hook( self,module, input, output):\n",
        "    print('Activations and Visualizations layer ',self.layer_counter+1,' module is ',module)\n",
        "    self.layer_counter+=1\n",
        "    with torch.no_grad():\n",
        "        norms = [i.norm() for i in output[0]]\n",
        "        ordered_indexes = np.argsort(norms)\n",
        "\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for index,i in enumerate(ordered_indexes[-18:][::-1]): \n",
        "      plt.subplot(3,6,index+1)\n",
        "      vis= transforms.ToPILImage()(output[0][i]).convert(\"RGB\")\n",
        "      enhancer = ImageEnhance.Contrast(vis)\n",
        "      enh_vis = enhancer.enhance(2)\n",
        "      plt.imshow(enh_vis,cmap='magma')\n",
        "      plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for index,i in enumerate(ordered_indexes[-18:][::-1]):\n",
        "      self.x.grad=None\n",
        "      self.zero_grad()\n",
        "      output[0][i].sum().backward(retain_graph=True)\n",
        "      plt.subplot(3,6,index+1)\n",
        "      # Try out various clipping/normalization methods\n",
        "      vis= transforms.ToPILImage()(self.x.grad[0]).convert(\"RGB\")\n",
        "      enhancer = ImageEnhance.Contrast(vis)\n",
        "      enh_vis = enhancer.enhance(2)\n",
        "      plt.imshow(enh_vis)\n",
        "      plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.layer_counter=0\n",
        "    x.requires_grad=True\n",
        "    self.x=x\n",
        "    \n",
        "    return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-yYmf_TGzsC"
      },
      "source": [
        "rn_backprop = VanillaBackprop(rn)\n",
        "logits= rn_backprop(input_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_O8jDbaH7td"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}